{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funasr import AutoModel\n",
    "import re\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 语音转写+端点检测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_asr = r\"E:\\LargeModel\\Speech_Synthesis\\GPT_Sovits\\GPT-SoVITS-beta\\GPT-SoVITS-beta_fast_inference_0316\\tools\\asr\\models\\speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch\"\n",
    "path_vad = r\"E:\\LargeModel\\Speech_to_Text\\Funasr\\iic\\speech_fsmn_vad_zh-cn-16k-common-pytorch\"\n",
    "path_punc=r\"E:\\LargeModel\\Speech_to_Text\\Funasr\\iic\\punc_ct-transformer_zh-cn-common-vocab272727-pytorch\"\n",
    "model = AutoModel(\n",
    "    model               = path_asr,\n",
    "    model_revision      = \"v2.0.4\",\n",
    "    vad_model           = path_vad,\n",
    "    vad_model_revision  = \"v2.0.4\",\n",
    "    # punc_model          = path_punc,\n",
    "    # punc_model_revision = \"v2.0.4\",\n",
    ")\n",
    "\n",
    "input_file_name = r'D:\\Desktop\\人声.wav'\n",
    "text0 = model.generate(input=input_file_name)[0][\"text\"]\n",
    "help(AutoModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 标点恢复"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punc_model = AutoModel(model=path_punc, model_revision=\"v2.0.4\")\n",
    "punc_res = punc_model.generate(input=text0)[0]['text']\n",
    "print(punc_res)\n",
    "\n",
    "#每当有端点时在text0的基础上添加一个空格\n",
    "split = '[,.?!;:。；：？！，\\n]'\n",
    "punc_res_copy =punc_res\n",
    "punc_text=''\n",
    "for subtext in text0.split(' '):\n",
    "    while(len(subtext)>0):\n",
    "        temp = re.search(split,  punc_res_copy)\n",
    "        if not temp:\n",
    "            punc_text += subtext\n",
    "            punc_text += ' '\n",
    "            break\n",
    "        left, right = temp.span()\n",
    "        if  left > len(subtext):\n",
    "            right = len(subtext)\n",
    "            subtext=''\n",
    "        else:\n",
    "            subtext = subtext[left:]\n",
    "        punc_text += punc_res_copy[:right]\n",
    "        punc_text += ' '\n",
    "        punc_res_copy = punc_res_copy[right:]\n",
    "text0 = punc_text\n",
    "del punc_text, punc_res_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 时间戳检测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#时间戳后面的文字是否带标点\n",
    "include_punc=True\n",
    "no_endpoint = re.sub(split, '', text0).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel(model=\"E:\\LargeModel\\Speech_to_Text\\Funasr\\iic\\speech_timestamp_prediction-v1-16k-offline\", model_revision=\"v2.0.4\")\n",
    "wav_file = input_file_name\n",
    "# text_file = r\"E:\\LargeModel\\Speech_to_Text\\Funasr\\yueer.txt\"\n",
    "res = model.generate(input=(wav_file, no_endpoint), data_type=(\"sound\", \"text\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_convert(tsecond):\n",
    "    '''\n",
    "    将秒转为时:分:秒,毫秒的形式\n",
    "    '''\n",
    "    ms = tsecond-int(tsecond)\n",
    "    tsecond = int(tsecond)\n",
    "    hour = tsecond//3600\n",
    "    minute = tsecond%3600//60\n",
    "    second = tsecond%60\n",
    "    return(str(hour).rjust(2,'0')+':'+str(minute).rjust(2,'0')+':'+str(second).rjust(2,'0')+','+str(round(ms*1000)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 字幕和时间轴合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res:list\n",
    "# res[0]:dict:{key, text, timestamp}\n",
    "tstamp = res[0]['timestamp']\n",
    "no_endpoint = no_endpoint.split(' ')\n",
    "if include_punc:\n",
    "    stamptext = text0\n",
    "else:\n",
    "    stamptext = no_endpoint\n",
    "stamptext = stamptext.split(' ')\n",
    "now_stamp=0\n",
    "num=1\n",
    "# 在输入音频的文件位置生成一个同名的字幕文件\n",
    "srt_file = '.'.join(input_file_name.split('.')[:-1])+'.srt'\n",
    "orign = sys.stdout \n",
    "sys.stdout = open(srt_file, 'w')\n",
    "for index in range(len(no_endpoint)):\n",
    "    start = tstamp[now_stamp][0]/1000#ms->s\n",
    "    now_stamp += len(no_endpoint[index])\n",
    "    end= tstamp[now_stamp-1][1]/1000#ms->s\n",
    "    print(\"%d\\n%s --> %s\\n%s\" % (num, time_convert(start), time_convert(end), stamptext[index]))\n",
    "    num+=1\n",
    "sys.stdout = orign "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 说话人检测(检测两个音频是否是同一人说话)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TorchAudio is not built with sox extension\n",
    "from modelscope.pipelines import pipeline\n",
    "input_file_name = r\"E:\\LargeModel\\Speech_Synthesis\\GPT_SOVITS_0训练集\\自己的模型\\姬如千泷\\素材\\人声.wav\"\n",
    "sv_pipeline = pipeline(\n",
    "    task='speaker-verification',\n",
    "    model=\"E:\\LargeModel\\Speech_to_Text\\Funasr\\damo\\speech_campplus_sv_zh-cn_16k-common\",\n",
    "    model_revision='v1.0.0'\n",
    ")\n",
    "speaker1_a_wav = \"E:\\LargeModel\\Speech_Synthesis\\GPT_SOVITS_0训练集\\派蒙-效果并不好\\参考音频\\生气—呜哇好生气啊！不要把我跟一斗相提并论！.wav\"\n",
    "speaker1_b_wav = \"E:\\LargeModel\\Speech_Synthesis\\GPT_SOVITS_0训练集\\派蒙-效果并不好\\参考音频\\说话—既然罗莎莉亚说足迹上有元素力，用元素视野应该能很清楚地看到吧。.wav\"\n",
    "speaker2_a_wav = \"E:\\LargeModel\\Speech_Synthesis\\GPT_SOVITS_0训练集\\派蒙-效果并不好\\参考音频\\疑问—哇，这个，还有这个…只是和史莱姆打了一场，就有这么多结论吗？.wav\"\n",
    "\n",
    "result = sv_pipeline([speaker1_a_wav, speaker1_b_wav], thr=0.1)\n",
    "print(result['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 情感表征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_name = r'D:\\Desktop\\人声.wav'\n",
    "from modelscope.pipelines import pipeline\n",
    "from modelscope.utils.constant import Tasks\n",
    "\n",
    "'''\n",
    "Using the finetuned emotion recognization model\n",
    "rec_result contains {'feats', 'labels', 'scores'}\n",
    "\textract_embedding=False: 9-class emotions with scores\n",
    "\textract_embedding=True: 9-class emotions with scores, along with features\n",
    "\n",
    "9-class emotions:\n",
    "    0: angry\n",
    "    1: disgusted\n",
    "    2: fearful\n",
    "    3: happy\n",
    "    4: neutral\n",
    "    5: other\n",
    "    6: sad\n",
    "    7: surprised\n",
    "    8: unknown\n",
    "'''\n",
    "inference_pipeline = pipeline(\n",
    "    task=Tasks.emotion_recognition,\n",
    "    model=r\"E:\\LargeModel\\Speech_to_Text\\Funasr\\iic\\emotion2vec_base_finetuned\", model_revision=\"v2.0.4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_result = inference_pipeline(input_file_name, output_dir=\"./outputs\", granularity=\"utterance\", extract_embedding=False)\n",
    "result = list(map(lambda x:(rec_result[0]['labels'][x[0]],x[1]), sorted(enumerate(rec_result[0]['scores']),key=lambda x:x[1], reverse=True)))\n",
    "print(result[0][0])\n",
    "for content in result:\n",
    "    print(*content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
