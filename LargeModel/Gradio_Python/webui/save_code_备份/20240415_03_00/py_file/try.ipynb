{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Model\n",
    "import re,sys\n",
    "messages =[]\n",
    "cache_path = \"E:\\LargeModel\\Gradio_Python\\weibui\\cache\"\n",
    "mymodel = Model(cache_path,messages)\n",
    "mymodel.disply_inf=False\n",
    "bot = mymodel.bot\n",
    "history=[[]]\n",
    "while(True):\n",
    "    question=input()\n",
    "    if len(question)==0:\n",
    "        break\n",
    "    history +=[[question, None]]\n",
    "    print('user:', question)\n",
    "    print('Bot：', end='')\n",
    "    for word in bot(history):\n",
    "        print('\\r', flush=True, end='')\n",
    "        print('Bot：'+word[-1][1],end='')\n",
    "        print('\\r', end='',flush=True)\n",
    "        print('\\r', end='',flush=True)\n",
    "        # sys.stdout.flush()\n",
    "    print()\n",
    "    history = word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 更改参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import json\n",
    "def gr_cut_params(model, max_epoch, cut_nepoch):\n",
    "    #生成cut_params\n",
    "    if model==\"ollama\":\n",
    "        #cut_nepoch随着max_epoch改变\n",
    "        max_epoch = gr.Slider(minimum=1,maximum=100,step=1,label='max_epoch',value=15,interactive=True, visible=True)\n",
    "        cut_nepoch = gr.Slider(minimum=0,maximum=15,step=1,label='cut_nepoch',value=5,interactive=True, visible=True)\n",
    "    elif model ==\"None\":\n",
    "        max_epoch = gr.Slider(visible=False)\n",
    "        cut_nepoch = gr.Slider(visible=False)\n",
    "    return(max_epoch, cut_nepoch)\n",
    "\n",
    "def cut_epoch(max_epoch, cut_nepoch):\n",
    "    #根据max_epoch生成cut_nepoch\n",
    "    cut_nepoch = gr.Slider(minimum=0,maximum=max_epoch,step=1,label='cut_nepoch',value=max(max_epoch//3,1),interactive=True, visible=True)\n",
    "    return(cut_nepoch)\n",
    "\n",
    "def save_cut_params(max_epoch, cut_nepoch):\n",
    "    cut_params = {\n",
    "        'max_epoch':max_epoch,\n",
    "        'cut_nepoch':cut_nepoch\n",
    "    }\n",
    "    with open('.\\cache\\cut_params.json','w') as file:\n",
    "        file.write(json.dumps(cut_params))\n",
    "    return('params has been saved')\n",
    "\n",
    "\n",
    "def gr_model_params(model,\n",
    "                num_predict,\n",
    "                temperature,\n",
    "                top_p,\n",
    "                top_k,\n",
    "                num_ctx,\n",
    "                repeat_penalty,\n",
    "                seed,\n",
    "                num_gpu,\n",
    "                stop\n",
    "                ):\n",
    "    if model =='None':\n",
    "        num_predict = gr.Slider(visible=False)\n",
    "        temperature = gr.Slider(visible=False)\n",
    "        top_p = gr.Slider(visible=False)\n",
    "        top_k = gr.Slider(visible=False)\n",
    "        num_ctx = gr.Slider(visible=False)\n",
    "        repeat_penalty = gr.Slider(visible=False)\n",
    "        seed = gr.Number(visible=False)\n",
    "        num_gpu = gr.Number(visible=False)\n",
    "        stop = gr.Textbox(visible=False)\n",
    "    elif model ==\"ollama\":\n",
    "        ###num_predict, num_ctx都是2的幂次, num_gpu要判断是否为-1，stop要按\\n拆分--model.py---done\n",
    "        num_predict = gr.Slider(minimum=5,maximum=14,step=1,label='2^num_predict',value=7,interactive=True, visible=True)\n",
    "        temperature = gr.Slider(minimum=0.1,maximum=1,step=0.1,label='temperature',value=0.8,interactive=True, visible=True)\n",
    "        top_p = gr.Slider(minimum=0.1,maximum=1,step=0.1,label='top_p',value=0.9,interactive=True, visible=True)\n",
    "        top_k = gr.Slider(minimum=5,maximum=100,step=5,label='top_k',value= 40,interactive=True, visible=True)\n",
    "        num_ctx = gr.Slider(minimum=5,maximum=14,step=1,label='num_ctx',value=11,interactive=True, visible=True)\n",
    "        repeat_penalty = gr.Slider(minimum=0.3,maximum=2,step=0.1,label='repeat_penalty',value=1.1,interactive=True,visible=True)\n",
    "        seed = gr.Number(label='seed',value=0,visible=True)\n",
    "        num_gpu = gr.Number(label='num_gpu',value=-1,visible=True)\n",
    "        stop = gr.Textbox(label=\"shift+enter to input more stop text. default:AI assistant:\", value=\"AI assistant:\", lines=2, visible=True)\n",
    "\n",
    "    return(num_predict,\n",
    "            temperature,\n",
    "            top_p,\n",
    "            top_k,\n",
    "            num_ctx,\n",
    "            repeat_penalty,\n",
    "            seed,\n",
    "            num_gpu,\n",
    "            stop\n",
    "            )\n",
    "\n",
    "def save_params(\n",
    "                num_predict,\n",
    "                temperature,\n",
    "                top_p,\n",
    "                top_k,\n",
    "                num_ctx,\n",
    "                repeat_penalty,\n",
    "                seed,\n",
    "                num_gpu,\n",
    "                stop\n",
    "                ):\n",
    "    params = {'num_predict':num_predict, \n",
    "                'temperature':temperature,\n",
    "                'top_p':top_p,\n",
    "                'top_k':top_k,\n",
    "                'num_ctx':num_ctx,\n",
    "                'repeat_penalty':repeat_penalty,\n",
    "                'seed':seed,\n",
    "                'num_gpu':num_gpu,\n",
    "                'stop':stop\n",
    "            }\n",
    "    with open('.\\cache\\params.json','w') as file:\n",
    "        file.write(json.dumps(params))\n",
    "    return('params has been saved')\n",
    "    \n",
    "import gradio as gr\n",
    "import ollama\n",
    "def found_model_loader():\n",
    "    #待办：model_loader待传入json\n",
    "    model_loader = ['None', 'ollama']\n",
    "    model_loader = gr.Dropdown(model_loader, label='model_loader')\n",
    "    return(model_loader)\n",
    "\n",
    "def found_model():\n",
    "    model = gr.Dropdown(['None'], label='model')\n",
    "    return(model)\n",
    "\n",
    "def model_select_update(model_loader, model):\n",
    "    # model_loader与model的级联界面，借鉴自https://zhuanlan.zhihu.com/p/663411336\n",
    "    if model_loader=='None':\n",
    "        models_list = ['None']\n",
    "    elif model_loader =='ollama':\n",
    "        #待办：可以事先写入一个json文件\n",
    "        models_list = ['None']+[model['name'] for model in  ollama.list()['models']]\n",
    "    model = gr.Dropdown.update(choices=models_list)\n",
    "    return(model)\n",
    "\n",
    "def save_model(model):\n",
    "    with open(r'.\\cache\\model_val.txt', 'a', encoding='utf-8') as file:\n",
    "        file.write(model + \"\\n\") \n",
    "    return\n",
    "\n",
    "def save_model_loader(model_loader):\n",
    "    with open(r'.\\cache\\model_loader_val.txt', 'a', encoding='utf-8') as file:\n",
    "        file.write(model_loader + \"\\n\") \n",
    "    return    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from model import Model\n",
    "from load_model import found_model, found_model_loader, model_select_update, save_model,save_model_loader,save_promot_text\n",
    "from model_loader_params import gr_cut_params,gr_model_params,cut_epoch,save_params,save_cut_params\n",
    "\n",
    "with gr.Blocks() as h:\n",
    "   promot_text = gr.Textbox(label='promot_text', value=\"\", lines=1)\n",
    "   promot_text.submit(save_promot_text, inputs=[promot_text], outputs=None)\n",
    "\n",
    "   #选择框架\n",
    "   model_loader = found_model_loader()\n",
    "   #选择模型\n",
    "   model = found_model()\n",
    "   model_loader.change(model_select_update, inputs=[model_loader, model], outputs=[model])\n",
    "   #存储\n",
    "   model_loader.change(save_model_loader, inputs=[model_loader], outputs=None)\n",
    "   model.change(save_model, inputs=[model], outputs=None)\n",
    "\n",
    "   # #选择最大对话轮次，超过时裁剪对话记录\n",
    "   max_epoch, cut_nepoch = gr.Slider(visible=False),gr.Slider(visible=False)\n",
    "   model_loader.change(gr_cut_params, inputs=[model_loader, max_epoch, cut_nepoch], outputs=[max_epoch, cut_nepoch])\n",
    "   max_epoch.change(cut_epoch, inputs=[max_epoch, cut_nepoch], outputs=[cut_nepoch])\n",
    "   gr.Interface(save_cut_params, inputs=[max_epoch, cut_nepoch], outputs='text')\n",
    "\n",
    "   #选择模型参数---待办：参数用params代替\n",
    "   num_predict = gr.Slider(visible=False)\n",
    "   temperature = gr.Slider(visible=False)\n",
    "   top_p = gr.Slider(visible=False)\n",
    "   top_k = gr.Slider(visible=False)\n",
    "   num_ctx = gr.Slider(visible=False)\n",
    "   repeat_penalty = gr.Slider(visible=False)\n",
    "   seed = gr.Number(visible=False)\n",
    "   num_gpu = gr.Number(visible=False)\n",
    "   stop = gr.Textbox(visible=False)\n",
    "   model_loader.change(gr_model_params, inputs=[model_loader, num_predict,temperature,top_p,top_k,num_ctx, repeat_penalty, seed,num_gpu,stop], outputs=[num_predict,temperature,top_p,top_k,num_ctx, repeat_penalty, seed,num_gpu,stop])\n",
    "   #将参数值存储cache/params.json\n",
    "   gr.Interface(save_params, inputs=[num_predict,temperature,top_p,top_k,num_ctx, repeat_penalty, seed,num_gpu,stop], outputs='text')\n",
    "         # 读取参数\n",
    "h.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
