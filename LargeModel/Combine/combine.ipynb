{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio,wave\n",
    "import ollama\n",
    "import warnings\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "import torch\n",
    "import re\n",
    "import logging\n",
    "os.environ['HF_ENDPOINT']='hf-mirror.com'#hugging_face镜像\n",
    "# os.environ['HF_HUB_OFFLINE']='0'\n",
    "\n",
    "#清理内存\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
    "if hasattr(torch.cuda, 'empty_cache'):\n",
    "    torch.cuda.empty_cache()\n",
    "del torch\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gradio\n",
    "# sys.path.append('../Gradio_Python')\n",
    "# from gradio import predict\n",
    "\n",
    "# #文转语音\n",
    "# import pyttsx3\n",
    "# sys.path.append('../text_to_voice')\n",
    "# from pyttsx3_def import text_voice##\n",
    "# engine = pyttsx3.init()\n",
    "# engine = text_voice(engine,rate=130)\n",
    "# (r,ve,vs) = (engine.getProperty('rate'),engine.getProperty('volume'), engine.getProperty('voices'))\n",
    "# print('目前语速是：',r,'\\t','语音音量是:',ve)\n",
    "\n",
    "#导入语音合成模型\n",
    "sys.path.append('../Speech_Synthesis\\GPT_Sovits\\GPT-SoVITS-beta\\GPT-SoVITS-beta0306fix\\GPT_SoVITS')\n",
    "from inference_maple import handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#声音转文本模型加载\n",
    "model_choose='sherpa_onnx'#fast_whisper, whisper, sherpa_onnx\n",
    "# model_choose='faster_whisper'\n",
    "# model_choose='whisper'\n",
    "\n",
    "if model_choose=='faster_whisper':\n",
    "    sys.path.append('../voice_record')\n",
    "    from voice_record_def import listen#记录输入的语音\n",
    "    sys.path.append('../Speech_to_Text')\n",
    "    from Fast_whisper.Fast_whisper import fast_whisper_opt\n",
    "    from faster_whisper import WhisperModel\n",
    "\n",
    "    model_size = \"small\"\n",
    "    # model_size='large-v3'\n",
    "    download_root=r'E:\\LargeModel\\Speech_to_Text\\fast_whisper'\n",
    "    #当download_root与模型在同一个目录时候\n",
    "    model = WhisperModel(download_root+'\\\\'+model_size, device=\"cuda\", compute_type=\"float16\", download_root=download_root, local_files_only=False)\n",
    "\n",
    "#whisper十分垃圾\n",
    "elif model_choose=='whisper':\n",
    "    # #whisper\n",
    "    import whisper\n",
    "    sys.path.append('../voice_record')\n",
    "    from voice_record_def import listen#记录输入的语音\n",
    "    sys.path.append('../Speech_to_Text')\n",
    "    from Whisper.Whisper import whisper_output\n",
    "    model_path = r'E:\\LargeModel\\Speech_to_Text\\Whisper\\Model'\n",
    "    # model= whisper.load_model(name='small', download_root=model_path)#device='cuda:0'\n",
    "    model= whisper.load_model(name='small', download_root=model_path)#device='cuda:0'\n",
    "\n",
    "elif model_choose == 'sherpa_onnx':\n",
    "    import sherpa_onnx\n",
    "    path = 'E:/LargeModel/kaldi/sherpa_onnx_model/stream_zipformer'\n",
    "    sys.path.append(path)\n",
    "    import microphone_endpoint\n",
    "    args_dict = {  \n",
    "        'tokens': path+'/tokens.txt',  \n",
    "        'encoder': path+'/encoder-epoch-99-avg-1.onnx',  \n",
    "        'decoder': path+'/decoder-epoch-99-avg-1.onnx' ,  \n",
    "        'joiner': path+'/joiner-epoch-99-avg-1.onnx',    \n",
    "    } \n",
    "    #检查设备，顺便输出recongnizer\n",
    "    check_recognizer= microphone_endpoint.main(args_dict)\n",
    "else:\n",
    "    print('Choose unfound asr model！')\n",
    "\n",
    "\n",
    "#文本对话初始化\n",
    "text_text_model='gemma:2b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#语音合成模型初始化\n",
    "ref_wav_path=\"E:\\LargeModel\\Speech_Synthesis\\GPT_Sovits\\GPT-SoVITS-beta\\派蒙--官方给的\\参考音频\\说话—既然罗莎莉亚说足迹上有元素力，用元素视野应该能很清楚地看到吧。.wav\"\n",
    "prompt_text= '既然罗莎莉亚说足迹上有元素力，用元素视野应该能很清楚地看到吧。'\n",
    "prompt_language='zh'\n",
    "text= '晚上好，祝你有一个甜美的睡眠！'\n",
    "text_language='zh'\n",
    "ref_free=False\n",
    "stream=False\n",
    "start=False\n",
    "top_k=5\n",
    "top_p=1\n",
    "temperature=1\n",
    "how_to_cut='按中英文句末标识符切'#输入按照句号切分，结合gemma文本每一段转换一次使用\n",
    "sovits_path = r\"E:\\LargeModel\\Speech_Synthesis\\GPT_Sovits\\GPT-SoVITS-beta\\GPT-SoVITS-beta0306fix\\GPT_SoVITS\\pretrained_models\\s2G488k.pth\"\n",
    "gpt_path = r\"E:\\LargeModel\\Speech_Synthesis\\GPT_Sovits\\GPT-SoVITS-beta\\GPT-SoVITS-beta0306fix\\GPT_SoVITS\\pretrained_models\\s1bert25hz-2kh-longer-epoch=68e-step=50232.ckpt\"\n",
    "\n",
    "def tts(ref_wav_path, prompt_text, prompt_language, outword, text_language, sovits_path, gpt_path, start=start, top_k=top_k, top_p=top_p,temperature=temperature,how_to_cut=how_to_cut, ref_free = ref_free, stream=stream):\n",
    "    # 将标准输出流重定向到空设备\n",
    "    original_stdout =  sys.stdout\n",
    "    sys.stdout = open('log_maple.txt', 'w')\n",
    "    handle(ref_wav_path, prompt_text, prompt_language, outword, text_language, sovits_path, gpt_path, start=start, top_k=top_k, top_p=top_p,temperature=temperature,how_to_cut=how_to_cut, ref_free = ref_free, stream=stream)\n",
    "    sys.stdout = original_stdout\n",
    "    logging.getLogger().setLevel(logging.WARNING)\n",
    "\n",
    "\n",
    "tts(ref_wav_path, prompt_text, prompt_language, text, text_language, sovits_path, gpt_path, start=start, top_k=top_k, top_p=top_p,temperature=temperature,how_to_cut=how_to_cut, ref_free = ref_free, stream=stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#拜拜结束对话\n",
    "#对话前删除临时录音文件\n",
    "def delete_files(folder_path):\n",
    "    file_list = glob.glob(folder_path + '/*')\n",
    "    for file_path in file_list:\n",
    "        if os.path.isfile(file_path):\n",
    "            if '.' in file_path and file_path.split('.')[0].split('/')[-1][:4]=='test':\n",
    "                os.remove(file_path)\n",
    "\n",
    "# 调用函数删除文件夹中的文件\n",
    "audio_file_path = 'test'\n",
    "delete_files(audio_file_path)\n",
    "\n",
    "#语音提醒可以对话了\n",
    "begin_conversion =\"您好，您现在可以跟我进行对话了！\" \n",
    "\n",
    "tts(ref_wav_path, prompt_text, prompt_language, begin_conversion, text_language, sovits_path, gpt_path, start=start, top_k=top_k, top_p=top_p,temperature=temperature,how_to_cut=how_to_cut, ref_free = ref_free, stream=stream)\n",
    "\n",
    "#用户说一次话并得到机器回复即为一轮。用户一轮说话中每有一个相对长的间隔，则将前面的录音分割为独立的一段，这一段视为一波录音。\n",
    "outfile_name=audio_file_path+'\\\\test.mp3'\n",
    "lc =0 #对话轮次，用于录音文件的名称存储\n",
    "while(True):\n",
    "    filenum=0 #每轮对话的录音波次\n",
    "    tag=0 #一轮对话开始的标志\n",
    "    sign=0 #whisper类是否转换声音了\n",
    "    lc_result=''#每一轮次的语音输入结果\n",
    "\n",
    "    while(model_choose !='sherpa_onnx'):\n",
    "        temp=outfile_name.split('.')\n",
    "        #当前波次有说话时，若后面说话间隔超过delayTime时进行下一波次录音；若当前波次还没有语音，当时间间隔超过tendure时候结束本轮录音。\n",
    "\n",
    "        #增加用户的反应时间，3s太短了。\n",
    "        sign, tag=listen(temp[0]+str(lc)+str('-')+str(filenum)+'.'+temp[1], tag, delayTime=0.8, tendure=5, mindb = 2000)\n",
    "        # print(tag)\n",
    "        if sign==1:#一轮对话录音结束的标志,每轮最后一波不转文字\n",
    "            # if tag==0: #如果第一波录音算是反应时间\n",
    "            #     continue\n",
    "            break\n",
    "        else:\n",
    "            #将该波次语音转为文字\n",
    "            # print(1,end='')\n",
    "            open_audio_file = '/'.join(outfile_name.split('//'))\n",
    "            open_audio_file=open_audio_file.split('.')\n",
    "            open_audio_file = open_audio_file[0]+str(lc)+str('-')+str(filenum)+'.'+open_audio_file[1]\n",
    "            # print(1,end='')\n",
    "            #---------------有声音，但是不是话语时候，fast_whisper会输出随机的词语。\n",
    "            if model_choose=='faster_whisper':\n",
    "                result = fast_whisper_opt(model, open_audio_file, initial_prompt=None)\n",
    "            #------------------whisper听到声音，但是这些声音是语气词无法翻译成话语时候会出错。\n",
    "            elif model_choose=='whisper':\n",
    "                result = whisper_output(model, open_audio_file, initial_prompt=None)\n",
    "            #当对话是刚开始的时候\n",
    "            # print(2,end='')\n",
    "    \n",
    "            lc_result += result\n",
    "            # lc_result += ','\n",
    "            if re.sub(r'[ .。！!?？-]','', result) in {'拜拜', '再见','byebye', 'Byebye' ,'掰掰'}:\n",
    "                print(result)\n",
    "                lc=-1\n",
    "                break\n",
    "\n",
    "            print(result, end='')\n",
    "        filenum += 1\n",
    "\n",
    "    #注意：每轮最后一波录音是空白\n",
    "\n",
    "    if sign==1 and tag==0:#当用户开始说话但是无语音输入时候，退出整个对话\n",
    "        print('\\n', '*'*40)\n",
    "        print(' '*5, '您的对话已超时，请重新对话！')\n",
    "        print('\\n', '*'*40)\n",
    "        break\n",
    "    #----------------------------------------------------------------------------#\n",
    "    #shepra_onnx asr模型，快，但是不那么准确\n",
    "    #输入声音但是无法翻译成话语时候会直接结束对话\n",
    "    if model_choose == 'sherpa_onnx':\n",
    "        # print('用户:  请输入你的问题')\n",
    "        last_result, lc_result = microphone_endpoint.main(args_dict, False, check_recognizer)\n",
    "        if last_result in {'拜拜'}:\n",
    "            lc=-1\n",
    "            result=last_result\n",
    "        \n",
    "        if len(lc_result)==0:\n",
    "            print('\\n', '*'*40)\n",
    "            print(' '*5, '您的对话已超时，请重新对话！')\n",
    "            print('\\n', '*'*40)\n",
    "            break\n",
    "\n",
    "    print()\n",
    "    #每轮的文本互动开始\n",
    "    print('Bot:',end='')\n",
    "    if lc==-1:\n",
    "        print(result)\n",
    "        break\n",
    "    lc +=1\n",
    "\n",
    "    #文本对话\n",
    "    stream = ollama.chat(\n",
    "        model=text_text_model,\n",
    "        messages=[{'role': 'user', 'content': lc_result}],\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    outword=\"\"\n",
    "    outword3=\"\"\n",
    "\n",
    "    max_chunk=4#每max_chunk轮播放一次\n",
    "    chunk_num=1\n",
    "    for chunk in stream:\n",
    "        outword3 = chunk['message']['content']\n",
    "        #打印对话内容\n",
    "        print(outword3, end='', flush=True)\n",
    "        outword += outword3\n",
    "        #语音播放\n",
    "        if outword3 in {'\\n', ',','.','。','，','!','！', '?', '？',':', '：'}:\n",
    "            #语音合成\n",
    "            # 将标准输出流重定向到空设备\n",
    "            #每max_chunk句就读一次\n",
    "            #chunk_num==max_chunk:\n",
    "\n",
    "            #每换一次行第一次。\n",
    "            if outword3=='\\n':\n",
    "                tts(ref_wav_path, prompt_text, prompt_language, outword, text_language, sovits_path, gpt_path, start=start, top_k=top_k, top_p=top_p, temperature=temperature,how_to_cut=how_to_cut, ref_free = ref_free, stream=stream)\n",
    "                # engine.say(outword)\n",
    "                outword=\"\"\n",
    "                chunk_num=1\n",
    "            #gemma是一个字一个chunk\n",
    "            else:\n",
    "                chunk_num +=1\n",
    "        # engine.runAndWait()\n",
    "\n",
    "    #把没凑够max_chunk的话读出来\n",
    "    if len(outword)!=0:\n",
    "        tts(ref_wav_path, prompt_text, prompt_language, outword, text_language, sovits_path, gpt_path, start=start, top_k=top_k, top_p=top_p,temperature=temperature,how_to_cut=how_to_cut, ref_free = ref_free, stream=stream)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
