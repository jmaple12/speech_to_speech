{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ollama聊天模板"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
    "if hasattr(torch.cuda, 'empty_cache'):\n",
    "      torch.cuda.empty_cache()\n",
    "\n",
    "import ollama\n",
    "import re\n",
    "models_list = [model['name'] for model in  ollama.list()['models']]\n",
    "print('models_list:\\n', models_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用户： test\n",
      "Bot：This is a test response from an AI language model. Is there anything specific you would like me to assist with?\n",
      "\n",
      "用户： test\n",
      "Bot：I'm sorry, but I am not able to generate content or complete tasks on my own without further guidance and information provided by the user.\n",
      "If there is something specific that you need assistance with, please let me know what it is so that I can provide you with the best possible advice and support.\n",
      "\n",
      "you are a help assistant\n",
      "None\n",
      "[{'role': 'system', 'content': 'you are a help assistant'}, {'role': 'user', 'content': 'test'}, {'role': 'assistant', 'content': \"I'm sorry, but I am not able to generate content or complete tasks on my own without further guidance and information provided by the user.\\nIf there is something specific that you need assistance with, please let me know what it is so that I can provide you with the best possible advice and support.\\n\"}]\n",
      "用户： test\n",
      "Bot：I'm sorry, but again, as an AI language model, my capabilities are limited to providing information and answering questions based on the input provided by the user. Therefore, in order for me to be able to assist you effectively, please let me know what specific task or question you need assistance with.\n",
      "Again, I apologize if this response has not met your expectations or requirements. Please feel free to provide any further guidance or information that may help me better understand and address your needs effectively.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#文档：https://github.com/ollama/ollama/blob/main/docs/modelfile.md\n",
    "#ollama会自动调用一些共享内存，即使设置了无系统回退也会这样\n",
    "def process_input(question):\n",
    "  #gemma:2b, qwen:4b-chat-v1.5-q6_K，qwen:7b-chat, qwen:14b-chat\n",
    "  #gemma:7b-instruct-q8_0\n",
    "  messages.append({'role': 'user', 'content': question})\n",
    "  # messages[{'role': 'user', 'content': question}]\n",
    "  print('用户：', question)\n",
    "  print('Bot：', end='')\n",
    "  stream = ollama.chat(\n",
    "      model='qwen:4b-chat-v1.5-q6_K',#可以长多轮对话，速度也挺快\n",
    "      # model='qwen:7b-chat-v1.5-q5_0',#跟14b一样对话轮次少\n",
    "      # model ='qwen:32b-chat-v1.5-q4_0', #30s一个字\n",
    "      # model='qwen:14b-chat',#CPU运行可以多轮，速度可以接受\n",
    "      # model='qwen:32b-chat-v1.5-q2_K',\n",
    "      messages=messages,\n",
    "      stream=True,\n",
    "      #option中的参数含义：https://github.com/ggerganov/llama.cpp/tree/master/examples/main#number-of-tokens-to-predict\n",
    "      options={\n",
    "      'num_predict': 512,#生成的最大tokens数,default:128\n",
    "      'temperature': 0.7,#default:0.8\n",
    "      # 'top_p':0.9,#default 0.9,\n",
    "      # 'top_k':20#defalut:40\n",
    "      'num_ctx':2048,#default=2048\n",
    "      'num_gpu':6,#使用gpu运行模型的层数，num_gpu=0时不使用gpu，为1的时候使用gpu内存。\n",
    "      'repeat_penalty':1.2, #default=1.1\n",
    "      'stop':[\"AI assistant:\"],#对话停止的输入\n",
    "      }\n",
    "  )\n",
    "  for chunk in stream:\n",
    "    result = chunk['message']['content']\n",
    "    result = re.sub('\\n+','\\n', result) \n",
    "    yield result\n",
    "\n",
    "messages =[]\n",
    "#加上system很快爆内存\n",
    "messages = [{'role':'system','content':'you are a help assistant'}]\n",
    "def get_system(messages):\n",
    "  cur_system = [message['content'] for message in messages if message['role']==\"system\"]\n",
    "  if cur_system:\n",
    "    cur_system = cur_system[-1]\n",
    "  else:\n",
    "    cur_system =None\n",
    "  return(cur_system)\n",
    "while (True):\n",
    "  question = input()\n",
    "  if len(question)==0:\n",
    "    break\n",
    "  # 查看当前对话轮次\n",
    "  epoch = [1 for message in messages if message['role']==\"assistant\"]\n",
    "  epoch = len(epoch)\n",
    "  #裁剪\n",
    "  if epoch >=15:\n",
    "    #获取最新的system:假设记录里面有多个system\n",
    "    curr_system = get_system(messages)\n",
    "    #定义裁剪的轮速\n",
    "    cut_num =4\n",
    "    now_num = 1\n",
    "    mess_ind =0\n",
    "    if cut_num>epoch:\n",
    "      print('Error!,裁剪轮次超过对话记录！')\n",
    "      break\n",
    "    while(now_num<=cut_num):\n",
    "      # print(now_num, mess_ind, messages[mess_ind]['role'])\n",
    "      if messages[mess_ind]['role'] == 'assistant':\n",
    "        now_num +=1\n",
    "        mess_ind += 1\n",
    "        continue\n",
    "      mess_ind += 1\n",
    "      \n",
    "    messages = messages[mess_ind:].copy()\n",
    "    #如果system的记录被删除了，则在开头加上\n",
    "    if (not get_system(messages))&(curr_system is not None):\n",
    "      messages.insert(0, {'role':'system','content':curr_system})\n",
    "\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
    "    if hasattr(torch.cuda, 'empty_cache'):\n",
    "      torch.cuda.empty_cache()\n",
    "  if len(question)==0:\n",
    "    break\n",
    "  outword=\"\"\n",
    "  for res in process_input(question):\n",
    "    outword+= res\n",
    "    print(res, end='', flush=True)\n",
    "  # print(stream['message']['content'],flush=True)\n",
    "  messages.append({'role': 'assistant','content':outword})\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #存储聊天记录\n",
    "# len(messages)##28>=24的时候应该裁剪\n",
    "import pickle\n",
    "# with open(r'E:\\LargeModel\\Language_Model\\Text_generation\\Gemma\\chat_message.pkl', 'wb') as save_file:\n",
    "#     pickle.dump(messages, save_file)\n",
    "\n",
    "#读取pickle\n",
    "with open(r'E:\\LargeModel\\Language_Model\\Text_generation\\Gemma\\chat_message.pkl', 'rb') as save_file:\n",
    "    messages = pickle.load(save_file)\n",
    "print(len(messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 常规输入模型转为Promot格式--ollama似乎不需要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(input_list, is_translate=True, is_input=True):\n",
    "    '''\n",
    "    将input_list转为Promot格式\n",
    "    is_translate:是否进行转换\n",
    "    '''\n",
    "    if not is_translate:\n",
    "        return(input_list)\n",
    "    input_list_copy = input_list.copy()\n",
    "    start = \"<|im_start|>\"\n",
    "    end= \"<|im_end|>\"\n",
    "    for index in range(len(input_list_copy)):\n",
    "        if type(input_list_copy[index]) == dict:\n",
    "            input_list_copy[index]=(f\"{start}{input_list_copy[index]['role']}\\n{input_list_copy[index]['content']}{end}\\n\")\n",
    "    if is_input:\n",
    "        input_list_copy.append(f\"{start}{'assistant'}\\n\")\n",
    "    return(input_list_copy)\n",
    "\n",
    "messages = [{'role':'system','content':'你被邀请来陪用户聊天'}]\n",
    "messages.append({'role': 'user', 'content': \"你能跟我聊天吗\"})\n",
    "print(''.join(translate(messages)))\n",
    "print('*'*40)\n",
    "outword='''对不起，我不知道你的名字是什么。'''\n",
    "messages.append({'role': 'assistant','content':outword})\n",
    "print(''.join(translate(messages, is_input=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ai回复并发声"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def text_voice(engine, rate=-1, volume=-1, voice=-1):\n",
    "    '''\n",
    "    三个参数：\n",
    "    rate:速率\n",
    "    volume:音量，【0,1】\n",
    "    voice:声音，0为男声，1为女声\n",
    "    '''\n",
    "    if rate!=-1:\n",
    "        engine.setProperty('rate', rate)\n",
    "    if volume!=-1:\n",
    "        engine.setProperty('volume',volume)\n",
    "    if voice!=-1:\n",
    "        voices = engine.getProperty('voices')       \n",
    "        engine.setProperty('voice', voices[voice].id)\n",
    "    return engine\n",
    "\n",
    "#pyttsx3无法说话\n",
    "import pyttsx3\n",
    "engine = pyttsx3.init()\n",
    "engine = text_voice(engine, rate=130)\n",
    "(r,ve,vs) = (engine.getProperty('rate'),engine.getProperty('volume'), engine.getProperty('voices'))\n",
    "print('目前语速是：',r,'\\t','语音音量是:',ve)\n",
    "\n",
    "\n",
    "import ollama\n",
    "# import speech\n",
    "stream = ollama.chat(\n",
    "    model='gemma:2b',\n",
    "    messages=[{'role': 'user', 'content': 'do you have a happy memorary'}],\n",
    "    stream=True,\n",
    ")\n",
    "outword2=\"\"\n",
    "outword=\"\"\n",
    "outword3=\"\"\n",
    "\n",
    "for chunk in stream:\n",
    "    outword3 = chunk['message']['content']\n",
    "    outword2 += outword3\n",
    "    outword += outword3\n",
    "    #语音播放\n",
    "    if outword3 in {',','.','。','，','!','！', '?', '？',':', '：'}:\n",
    "        outword = outword.replace('\\n','').replace('*','')\n",
    "        # speech.say(outword)\n",
    "        engine.say(outword)\n",
    "        \n",
    "        outword=\"\"\n",
    "    engine.runAndWait()\n",
    "    #打印对话内容\n",
    "    print(outword3, end='', flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
