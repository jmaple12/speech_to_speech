# 本地语音对话

    我在本项目里结合了多个AI模型，来实现语音对话的功能，当然硬将多个不同功能的模型糅合在一起也会浪费一些计算机性能。这里我仅仅分享自己摘windows系统中是如何搭建多个AI模型的，然后缝合的代码在[languagemodel](https://github.com/jmaple12/speech_to_speech/tree/main/LargeModel)里面。为了使用AI模型，首先得现在GPU版本的torch以及对应的cudnn。  
　　下面我将要介绍[languagemodel](https://github.com/jmaple12/speech_to_speech/tree/main/LargeModel)里各个子文件夹的意义以及需要下载的东西，我主要展示自己的缝合代码。

## 录音
　　我定义了“Listen”函数来实现自动录音，且当人声停止的时候，自动停止录音。这个函数在这里[voice_record_def.py](https://github.com/jmaple12/speech_to_speech/blob/main/LargeModel/voice_record/voice_record_def.py)。
```        
def listen(WAVE_OUTPUT_FILENAME, tag, delayTime=2, tendure=2, mindb = 500):
return(sign, tag)
```
　　其中**WAVE_OUTPUT_FILENAME**是设定的存放录音文件的路径，“Listen”里的“sign”变量是用来记录该次录音是否记录到了声音，当“sign=1”时表明本次录音什么声音都没有录到。**mindb**表示能录到的最小分贝值。如果之前录到了声音，且之后连续**delayTime** 秒没有接收到新的声音，录音会停止， 如果全程持续**tendure**秒没有录到声音，录音也会停止，并且此时“sign=1”来表示整个录音过程都没有录到声音。 

  在[LargeModel/Combine/combine.ipynb](https://github.com/jmaple12/speech_to_speech/blob/main/LargeModel/Combine/combine.ipynb)的录音板块中，一轮录音会执行多次“listen”函数。我们输入语音的时候，每次稍长时间的停顿（超过**delayTime**秒），“Listen”就会记录一次录音结果，将此波次的录音音频存储到指定文件夹[test](https://github.com/jmaple12/speech_to_speech/tree/main/LargeModel/Combine/test)，并执行下一波次的“listen”函数，直到“listen”函数接收到一波次时长超过**tendure** 秒的空白录音，到此一轮录音结束。

  在[LargeModel/Combine/combine.ipynb](https://github.com/jmaple12/speech_to_speech/blob/main/LargeModel/Combine/combine.ipynb)里面，每有记录一波录音，专门的ASR模型会自动将其转为文本，并与之前转换的文本连接起来，在整轮录音及转换文字完成后送到文本生成模型中去。
  
## Sound to Text/ASR
### faster_whisper  
　　 这一部分我首先介绍语音识别模型Faster_whisper，它是对Whisper进行C语言的重新编译而得到的，与Whisper相比占用更低，速度更快。首先我们需要在[fast-whisper](https://github.com/SYSTRAN/faster-whisper)查看模型介绍，在[huggingface-large-v3](https://huggingface.co/Systran/faster-whisper-large-v3)或镜像站[mirror-large-v3](https://hf-mirror.com/Systran/faster-whisper-large-v3)下载模型，large-v3版本的faster_whisper在int8版本下大约需要3.5G显存，float16版本下大约需要4G多显存，如果显存低于4G，可以去下载它的small或者medium版本。需要注意large-v3的训练集里面有中文，其他低版本的训练集里面没有中文。

   faster_whisper转换地比较准确，甚至可以转换出标点符号，不过录音的时候如果不说话，它可能会转换处奇怪的话，或者出现错误。

### sherpa
    因为我的电脑性能较差，除了深度学习大模型，我也尝试了对设备要求较低的模型，比如这个onnx版本的sherpa。它是基于新一代kaldi（小米集团语音首席科学家Daniel Povey被称为kaldi之父）的机器学习语音识别模型。优点就是识别特别快，基本上能做到实时语音转换，并且它的有些预训练模型同时支持中英文转换。其次就是它是轻量级模型，在CPU上基本上就能实现实时转换，对机器要求低，甚至可以在安卓机上运行。但是缺点是与fast_whisper相比，它还不够准确，只能说对要求不高且设备性能有限的人来说够用，而且它无法转换出来标点符号。它的CPU版本安装比较容易，教程也比较多，但是GPU版本我至今没有安装成功，GPU版本的性能应该比CPU好很多。  

    安装Windows版本的sherpa_onnx可以在[sherpa_onnx](https://k2-fsa.github.io/sherpa/onnx/install/windows.html#bit-windows-x64)找到，根据[sherpa-onnx python](https://k2-fsa.github.io/sherpa/onnx/python/install.html#method-1-from-pre-compiled-wheels)可以安装它的python扩展包，它的实时语音转换教程可以在[real-time-speech-recongition](https://k2-fsa.github.io/sherpa/onnx/python/real-time-speech-recongition-from-a-microphone.html)找到，除此之外还有它的将音频文件转为文本的教程。它的预训练模型可以在[pretrained model github](https://github.com/k2-fsa/sherpa-onnx/releases/tag/asr-models)或[pretrained model ks-fsa](https://k2-fsa.github.io/sherpa/onnx/pretrained_models/index.html)下载。“新一代aldi”是他们的公众号，b站也有他们的账号，还有演示。  

    为了在python中直接调用语音转文字的功能，在[LargeModel/kalid/sherpa_onnx/microphone_endpoint.py](https://github.com/jmaple12/speech_to_speech/blob/main/LargeModel/kalid/sherpa_onnx/microphone_endpoint.py)中，我修改了官方[python-api-examples/speech-recognition-from-microphone-with-endpoint-detection.py](https://github.com/k2-fsa/sherpa-onnx/blob/master/python-api-examples/speech-recognition-from-microphone-with-endpoint-detection.py)实时语音检测的部分代码。官方代码是用在cmd中的。在python中直接调用main函数就可以进行录音并输出转换的文字了。

### whisper
　　Besides, [fast_whisper.ipynb](https://github.com/jmaple12/speech_to_speech/blob/main/LargeModel/Speech_to_Text/Fast_whisper/fast_whisper.ipynb) can automatically create caption file(.srt) of an audio, it may reduce work for somebody.    

## Large language Model

　　And then we need to download large LLM, I use [Google Gemma](https://github.com/google/gemma_pytorch), and I use it through [ollama](https://github.com/ollama/ollama), in [ollama weibsite](https://ollama.com/), download its application, and execute below code in cmd
```
ollama run gemma:7b
or
ollama run gemma:2b
```
and then we can use the gemma model. 

## Text_to_Voice/Speech_Synthesis

### Text_to_Voice

　　**pyttsx3** can directly use the windows system voice package, and it consumes minimal resources but its voice is bad. 

### Speech_Synthesis

　　On the other hand, we can also use Speech_Synthesis Model to create our own sound, I use GPT_Sovits Model[GPT_Sovits Model](https://github.com/RVC-Boss/GPT-SoVITS),its new version for windows is here[windows Integration package0306](https://www.123pan.com/s/5tIqVv-GVRcv.html), it gives a package to process from audio cleaning to audio_model_train and model_inference. It contains a pretained model about Paimon's voice, I think it is enough, therefore, I use its pretained model and then I only focus on its inference section.   
  
  In order to use its inference model, we need to place the Integration package under the [GPT_Sovits](https://github.com/jmaple12/speech_to_speech/tree/main/LargeModel/Speech_Synthesis/GPT_Sovits) and place the [inference_maple.py](https://github.com/jmaple12/speech_to_speech/blob/main/LargeModel/Speech_Synthesis/GPT_Sovits/inference_maple.py) into the Integration package folder which contains **inference_webui.py** , and edit the variables **all_path** according to yourself filepath in the line 14 of the [inference_maple.py](https://github.com/jmaple12/speech_to_speech/blob/main/LargeModel/Speech_Synthesis/GPT_Sovits/inference_maple.py).      

　　Besides, [gpt_sovits_api.ipynb](https://github.com/jmaple12/speech_to_speech/blob/main/LargeModel/Speech_Synthesis/GPT_Sovits/gpt_sovits_api.ipynb) gives two ways to inference, and second way can directly use its inference section without downloading python module to yourself environment, it only need to run the api.py in cmd before excute the code, and api may cause more time delay.    

　　Notice：GPT_Sovits has its own environment, if we use the first way in  [gpt_sovits_api.ipynb](https://github.com/jmaple12/speech_to_speech/blob/main/LargeModel/Speech_Synthesis/GPT_Sovits/gpt_sovits_api.ipynb)  or [combine.ipynb](https://github.com/jmaple12/speech_to_speech/blob/main/LargeModel/Combine/combine.ipynb) we need to install some package into ourself environment like package: cn2an, pypinyin, jieba_fast, pyopenjtalk, g2p_en, ffmpeg-python and so on.

## Final 

　　open [combine.ipynb](https://github.com/jmaple12/speech_to_speech/blob/main/LargeModel/Combine/combine.ipynb), download some python package, modify variables in the third bloack 
```
model_size, download_root, model_path, text_text_model
```
and variables in fourth block 
  ```
ref_wav_path, prompt_text, prompt_language, text, text_language, sovits_path, gpt_path
```
according to your condition.

## Summarize

　　I mainly make a framework which suture several AI model to achieve speech to speech, and I focus on the localization model. Because this issue need to run at least 2 AI model at the same time, it requires large GPU video memory, and it will be worse than the speech conversation AI model. My GPU only has 4G video memory, so I only run the framework in a low level, fast_whisper can't translate my voice well and the gpt_sovits slowly deal with a sentense per 3 seconds. if you have higher computer congifure, you can try to use larger fast_whisper and Gemma  model or other outstanding AI model.  
  
　　of course, we can also place api of internet AI model in this framework, it may achieve much better performance may cause more time delay. I find Ernie Bot app has speech conversation function, and it is good.    
  
  　I hope an AI loudspeaker box which can communicate with human fluently and intelligently as it is a man appear in 5 years, I really need them.


    
